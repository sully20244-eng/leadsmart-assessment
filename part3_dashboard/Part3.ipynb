{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0862f68e-3e8a-4689-9e1e-97cd7df29b4a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b172595-fefa-4ade-ad58-e96ee5cc5ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports & basic setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b13e96-11a8-48e3-9210-06a93931db65",
   "metadata": {},
   "source": [
    "# Intro & Business Framing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c9703f-61f4-41b6-b273-db28491ad360",
   "metadata": {},
   "source": [
    "# Part 3 – Predictive Intelligence: Lead Prioritization\n",
    "\n",
    "## 1. Business Problem\n",
    "\n",
    "Sales teams are overwhelmed with leads coming from Facebook campaigns.  \n",
    "Some leads will eventually become qualified opportunities or closed deals, but many will not.  \n",
    "Treating all leads equally wastes time and slows down response times for high-value leads.\n",
    "\n",
    "## 2. Objective\n",
    "\n",
    "Build a **binary classification model** that estimates the probability that a new lead will become a **converted / high-quality lead**.\n",
    "\n",
    "We will use this model to:\n",
    "\n",
    "- Prioritise **which leads should be called first** (High / Medium / Low priority).\n",
    "- Help sales teams focus their time on the leads with the **highest likelihood of conversion**.\n",
    "- Provide a score that can be surfaced in dashboards or CRM workflows.\n",
    "\n",
    "## 3. Conversion Definition\n",
    "\n",
    "For this analysis, we define a **converted lead** as a lead whose final status is one of:\n",
    "\n",
    "- `QUALIFIED`\n",
    "- `HIGH_INTEREST`\n",
    "- `MEETING_DONE`\n",
    "- `RESALE_REQUEST`\n",
    "- `DONE_DEAL`\n",
    "- `ALREADY_BOUGHT`\n",
    "\n",
    "All other final statuses are treated as **not converted**.\n",
    "\n",
    "> Note: With more business input, the exact list of “conversion” statuses can be adjusted (e.g. only `DONE_DEAL` and `ALREADY_BOUGHT`).\n",
    "\n",
    "## 4. Modeling Goal\n",
    "\n",
    "- Input: lead-level information (lead metadata, campaign configuration, historical campaign performance).\n",
    "- Output: a **conversion probability** and a **priority bucket** (High / Medium / Low) for each lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed973e14-5467-40ec-82df-0f0408d34af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b0d1e-e42f-43e9-9fd4-af41da42c94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35c00f9f-21c1-40d5-b461-733f70da6ff3",
   "metadata": {},
   "source": [
    "# 2) Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "214cb2e3-2e95-4926-b7f1-39a7539d1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling dataset shape: (56965, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_id</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>lead_status</th>\n",
       "      <th>added_date</th>\n",
       "      <th>date</th>\n",
       "      <th>is_converted</th>\n",
       "      <th>user_id</th>\n",
       "      <th>project_name</th>\n",
       "      <th>daily_budget</th>\n",
       "      <th>hist_spend</th>\n",
       "      <th>hist_clicks</th>\n",
       "      <th>hist_impr</th>\n",
       "      <th>hist_ctr</th>\n",
       "      <th>hist_cpc</th>\n",
       "      <th>hist_cpm</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>name_length</th>\n",
       "      <th>email_length</th>\n",
       "      <th>phone_length</th>\n",
       "      <th>user_id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77930</td>\n",
       "      <td>6496</td>\n",
       "      <td>Vicky Mohr Sr.</td>\n",
       "      <td>smith.oren@example.org</td>\n",
       "      <td>+2015628437879</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2024-06-01 17:00:40</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>Il Cazar Safia north coast</td>\n",
       "      <td>2000</td>\n",
       "      <td>2137.86</td>\n",
       "      <td>156.0</td>\n",
       "      <td>8019.0</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>13.704231</td>\n",
       "      <td>266.599327</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77942</td>\n",
       "      <td>6496</td>\n",
       "      <td>Clovis Mueller</td>\n",
       "      <td>toy.korey@example.net</td>\n",
       "      <td>+1-615-909-5723</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2024-06-01 18:22:00</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>Il Cazar Safia north coast</td>\n",
       "      <td>2000</td>\n",
       "      <td>2137.86</td>\n",
       "      <td>156.0</td>\n",
       "      <td>8019.0</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>13.704231</td>\n",
       "      <td>266.599327</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77964</td>\n",
       "      <td>6493</td>\n",
       "      <td>Austin Ondricka II</td>\n",
       "      <td>hveum@example.org</td>\n",
       "      <td>+2014034035897</td>\n",
       "      <td>NOT_QUALIFIED</td>\n",
       "      <td>2024-06-01 21:56:33</td>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>0</td>\n",
       "      <td>466</td>\n",
       "      <td>Azha North Coast Ras El Hekma</td>\n",
       "      <td>1200</td>\n",
       "      <td>689.19</td>\n",
       "      <td>418.0</td>\n",
       "      <td>6270.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.648780</td>\n",
       "      <td>109.918660</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77983</td>\n",
       "      <td>6500</td>\n",
       "      <td>Jaquan Kuhn</td>\n",
       "      <td>greenholt.elsa@example.com</td>\n",
       "      <td>310-242-6257</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2024-06-02 01:13:19</td>\n",
       "      <td>2024-06-02</td>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>sky ad - new cairo launch</td>\n",
       "      <td>1200</td>\n",
       "      <td>842.40</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1086.0</td>\n",
       "      <td>0.057090</td>\n",
       "      <td>13.587097</td>\n",
       "      <td>775.690608</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77988</td>\n",
       "      <td>6496</td>\n",
       "      <td>Prof. Grayson Collier</td>\n",
       "      <td>obie83@example.com</td>\n",
       "      <td>+2016402961934</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2024-06-02 01:41:56</td>\n",
       "      <td>2024-06-02</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>Il Cazar Safia north coast</td>\n",
       "      <td>2000</td>\n",
       "      <td>2137.86</td>\n",
       "      <td>156.0</td>\n",
       "      <td>8019.0</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>13.704231</td>\n",
       "      <td>266.599327</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lead_id  campaign_id                   name                       email  \\\n",
       "0    77930         6496         Vicky Mohr Sr.      smith.oren@example.org   \n",
       "1    77942         6496         Clovis Mueller       toy.korey@example.net   \n",
       "2    77964         6493     Austin Ondricka II           hveum@example.org   \n",
       "3    77983         6500            Jaquan Kuhn  greenholt.elsa@example.com   \n",
       "4    77988         6496  Prof. Grayson Collier          obie83@example.com   \n",
       "\n",
       "             phone    lead_status          added_date        date  \\\n",
       "0   +2015628437879        UNKNOWN 2024-06-01 17:00:40  2024-06-01   \n",
       "1  +1-615-909-5723        UNKNOWN 2024-06-01 18:22:00  2024-06-01   \n",
       "2   +2014034035897  NOT_QUALIFIED 2024-06-01 21:56:33  2024-06-01   \n",
       "3     310-242-6257        UNKNOWN 2024-06-02 01:13:19  2024-06-02   \n",
       "4   +2016402961934        UNKNOWN 2024-06-02 01:41:56  2024-06-02   \n",
       "\n",
       "   is_converted  user_id                   project_name  daily_budget  \\\n",
       "0             0      318     Il Cazar Safia north coast          2000   \n",
       "1             0      318     Il Cazar Safia north coast          2000   \n",
       "2             0      466  Azha North Coast Ras El Hekma          1200   \n",
       "3             0      217      sky ad - new cairo launch          1200   \n",
       "4             0      318     Il Cazar Safia north coast          2000   \n",
       "\n",
       "   hist_spend  hist_clicks  hist_impr  hist_ctr   hist_cpc    hist_cpm  \\\n",
       "0     2137.86        156.0     8019.0  0.019454  13.704231  266.599327   \n",
       "1     2137.86        156.0     8019.0  0.019454  13.704231  266.599327   \n",
       "2      689.19        418.0     6270.0  0.066667   1.648780  109.918660   \n",
       "3      842.40         62.0     1086.0  0.057090  13.587097  775.690608   \n",
       "4     2137.86        156.0     8019.0  0.019454  13.704231  266.599327   \n",
       "\n",
       "   day_of_week  month  name_length  email_length  phone_length user_id_str  \n",
       "0            5      6           14            22            14         318  \n",
       "1            5      6           14            21            15         318  \n",
       "2            5      6           18            17            14         466  \n",
       "3            6      6           11            26            12         217  \n",
       "4            6      6           21            18            14         318  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Data Loading & Preparation\n",
    "\n",
    "# 2.1 Load raw CSVs\n",
    "campaign_leads = pd.read_csv(\"campaign_leads.csv\")\n",
    "campaigns = pd.read_csv(\"campaigns.csv\")\n",
    "insights = pd.read_csv(\"insights.csv\")\n",
    "lead_status_changes = pd.read_csv(\"lead_status_changes.csv\")  # not heavily used in this first version\n",
    "\n",
    "# 2.2 Basic cleaning / renaming\n",
    "campaign_leads = campaign_leads.rename(columns={\"id\": \"lead_id\"})\n",
    "campaigns = campaigns.rename(columns={\"id\": \"campaign_id\"})\n",
    "\n",
    "# 2.3 Parse dates\n",
    "campaign_leads[\"added_date\"] = pd.to_datetime(campaign_leads[\"added_date\"])\n",
    "insights[\"created_at\"] = pd.to_datetime(insights[\"created_at\"])\n",
    "lead_status_changes[\"created_at\"] = pd.to_datetime(lead_status_changes[\"created_at\"])\n",
    "\n",
    "# Derive a pure date column (without time) where needed\n",
    "campaign_leads[\"date\"] = campaign_leads[\"added_date\"].dt.date\n",
    "insights[\"date\"] = insights[\"created_at\"].dt.date\n",
    "\n",
    "# 2.4 Define conversion label based on final lead_status\n",
    "conversion_statuses = [\n",
    "    \"DONE_DEAL\",\n",
    "    \"ALREADY_BOUGHT\",\n",
    "    \"RESALE_REQUEST\",\n",
    "    \"MEETING_DONE\",\n",
    "    \"HIGH_INTEREST\",\n",
    "    \"QUALIFIED\",\n",
    "]\n",
    "\n",
    "campaign_leads[\"is_converted\"] = campaign_leads[\"lead_status\"].isin(conversion_statuses).astype(int)\n",
    "\n",
    "# 2.5 Join leads with campaign configuration\n",
    "leads_cfg = campaign_leads.merge(\n",
    "    campaigns[[\"campaign_id\", \"user_id\", \"project_name\", \"daily_budget\"]],\n",
    "    on=\"campaign_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 2.6 Aggregate insights per campaign (historical performance)\n",
    "insights_agg = (\n",
    "    insights.groupby(\"campaign_id\", as_index=False)\n",
    "    .agg(\n",
    "        hist_spend=(\"spend\", \"sum\"),\n",
    "        hist_clicks=(\"clicks\", \"sum\"),\n",
    "        hist_impr=(\"impressions\", \"sum\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Avoid division by zero\n",
    "insights_agg[\"hist_ctr\"] = np.where(\n",
    "    insights_agg[\"hist_impr\"] > 0,\n",
    "    insights_agg[\"hist_clicks\"] / insights_agg[\"hist_impr\"],\n",
    "    0.0\n",
    ")\n",
    "insights_agg[\"hist_cpc\"] = np.where(\n",
    "    insights_agg[\"hist_clicks\"] > 0,\n",
    "    insights_agg[\"hist_spend\"] / insights_agg[\"hist_clicks\"],\n",
    "    0.0\n",
    ")\n",
    "insights_agg[\"hist_cpm\"] = np.where(\n",
    "    insights_agg[\"hist_impr\"] > 0,\n",
    "    insights_agg[\"hist_spend\"] / insights_agg[\"hist_impr\"] * 1000,\n",
    "    0.0\n",
    ")\n",
    "\n",
    "# 2.7 Merge aggregated performance back to leads\n",
    "leads_model = leads_cfg.merge(\n",
    "    insights_agg,\n",
    "    on=\"campaign_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Replace missing performance with zeros (campaign with no insights)\n",
    "for col in [\"hist_spend\", \"hist_clicks\", \"hist_impr\", \"hist_ctr\", \"hist_cpc\", \"hist_cpm\"]:\n",
    "    leads_model[col] = leads_model[col].fillna(0.0)\n",
    "\n",
    "# 2.8 Feature engineering: date parts & text lengths\n",
    "leads_model[\"day_of_week\"] = leads_model[\"added_date\"].dt.weekday  # 0=Monday\n",
    "leads_model[\"month\"] = leads_model[\"added_date\"].dt.month\n",
    "\n",
    "leads_model[\"name_length\"] = leads_model[\"name\"].astype(str).str.len()\n",
    "leads_model[\"email_length\"] = leads_model[\"email\"].astype(str).str.len()\n",
    "leads_model[\"phone_length\"] = leads_model[\"phone\"].astype(str).str.len()\n",
    "\n",
    "# Convert user_id to string for categorical encoding\n",
    "leads_model[\"user_id_str\"] = leads_model[\"user_id\"].astype(str)\n",
    "\n",
    "# Keep only rows with non-null label\n",
    "leads_model = leads_model.dropna(subset=[\"is_converted\"])\n",
    "\n",
    "print(\"Modeling dataset shape:\", leads_model.shape)\n",
    "leads_model.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72ce76-be16-4512-bbc0-dffc2878d595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564cb66f-2c27-4f21-9932-4f18093273d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f35fc496-e2b6-4a43-8301-f046cfb15eef",
   "metadata": {},
   "source": [
    "# EDA for Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5e567-af19-4a75-a89f-73551c333175",
   "metadata": {},
   "source": [
    "## 3. Exploratory Analysis of the Modeling Dataset\n",
    "\n",
    "In this section we check:\n",
    "\n",
    "- Class balance: how many converted vs. non-converted leads.\n",
    "- Basic statistics of key numeric features.\n",
    "- Sanity checks on important categorical features (project, user, lead_status).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba9e9cf2-14ec-4838-9eed-7a25cec44c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts (0 = not converted, 1 = converted):\n",
      "is_converted\n",
      "0    52529\n",
      "1     4436\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class ratio (%):\n",
      "is_converted\n",
      "0    92.21\n",
      "1     7.79\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Top projects:\n",
      "project_name\n",
      "Kings Way                        6023\n",
      "Plage                            5746\n",
      "Mountain View iCity October      4197\n",
      "Sarai                            2319\n",
      "Mountain View ICity New Cairo    1572\n",
      "WonderMarQ - WaterMarQ           1388\n",
      "Ora Solana West                  1095\n",
      "cityscape 2023                   1087\n",
      "solare - misr italia             1079\n",
      "Palm Hills New Cairo             1066\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top users (user_id):\n",
      "user_id\n",
      "468     5953\n",
      "199     5180\n",
      "466     3450\n",
      "411     1936\n",
      "1243    1828\n",
      "1019    1425\n",
      "1535    1135\n",
      "837      974\n",
      "1005     901\n",
      "365      900\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Lead status distribution:\n",
      "lead_status\n",
      "UNKNOWN          27582\n",
      "NEW_LEAD         14724\n",
      "QUALIFIED         4063\n",
      "NO_ANSWER         3124\n",
      "NOT_QUALIFIED     2821\n",
      "CALL_AGAIN         923\n",
      "FOLLOW_UP          921\n",
      "WHATSAPP           844\n",
      "LOW_BUDGET         547\n",
      "SWITCHED_OFF       440\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Class balance\n",
    "class_counts = leads_model[\"is_converted\"].value_counts()\n",
    "class_ratio = leads_model[\"is_converted\"].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class counts (0 = not converted, 1 = converted):\")\n",
    "print(class_counts)\n",
    "print(\"\\nClass ratio (%):\")\n",
    "print(class_ratio.round(2))\n",
    "\n",
    "# 3.2 Basic stats for numeric features\n",
    "numeric_cols_preview = [\n",
    "    \"daily_budget\", \"hist_spend\", \"hist_clicks\", \"hist_impr\",\n",
    "    \"hist_ctr\", \"hist_cpc\", \"hist_cpm\",\n",
    "    \"day_of_week\", \"month\",\n",
    "    \"name_length\", \"email_length\", \"phone_length\"\n",
    "]\n",
    "\n",
    "leads_model[numeric_cols_preview].describe().T\n",
    "\n",
    "# 3.3 Quick look at top categories\n",
    "print(\"\\nTop projects:\")\n",
    "print(leads_model[\"project_name\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nTop users (user_id):\")\n",
    "print(leads_model[\"user_id\"].value_counts().head(10))\n",
    "\n",
    "print(\"\\nLead status distribution:\")\n",
    "print(leads_model[\"lead_status\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e32ab38-eaf8-4a37-acac-01d36109fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total leads    : 56,965\n",
      "Positives (1)  : 4,436\n",
      "Negatives (0)  : 52,529\n",
      "Positive ratio : 7.79%\n"
     ]
    }
   ],
   "source": [
    "# 3.x – Check imbalance more explicitly\n",
    "\n",
    "pos = leads_model[\"is_converted\"].sum()\n",
    "neg = len(leads_model) - pos\n",
    "\n",
    "print(f\"Total leads    : {len(leads_model):,}\")\n",
    "print(f\"Positives (1)  : {pos:,}\")\n",
    "print(f\"Negatives (0)  : {neg:,}\")\n",
    "print(f\"Positive ratio : {pos / len(leads_model) * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec11be-7bee-4663-9ac9-9fcd823d15ca",
   "metadata": {},
   "source": [
    "# 4) Train/Test Split (90/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43fde3-8158-464c-a397-e1573925704b",
   "metadata": {},
   "source": [
    "## 4. Train / Test Split (90/10)\n",
    "\n",
    "We will:\n",
    "\n",
    "- Build a feature matrix `X` and label vector `y`.\n",
    "- Use a 90/10 split with stratification on the target to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9444fce-e9aa-4ae4-b42c-a024e3302e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (51268, 15)\n",
      "Test shape : (5697, 15)\n",
      "scale_pos_weight for XGBoost (train only) = 11.84\n",
      "\n",
      "===== Training: Baseline (Most Frequent) =====\n",
      "Accuracy: 0.9221\n",
      "ROC AUC : 0.5000\n",
      "PR AUC  : 0.0779\n",
      "\n",
      "===== Training: Logistic Regression =====\n",
      "Accuracy: 1.0000\n",
      "ROC AUC : 1.0000\n",
      "PR AUC  : 1.0000\n",
      "\n",
      "===== Training: Random Forest =====\n",
      "Accuracy: 0.9986\n",
      "ROC AUC : 1.0000\n",
      "PR AUC  : 1.0000\n",
      "\n",
      "===== Training: XGBoost =====\n",
      "Accuracy: 1.0000\n",
      "ROC AUC : 1.0000\n",
      "PR AUC  : 1.0000\n",
      "\n",
      "===== Training: AdaBoost =====\n",
      "Accuracy: 0.9932\n",
      "ROC AUC : 0.9996\n",
      "PR AUC  : 0.9955\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.993154</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.995516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline (Most Frequent)</td>\n",
       "      <td>0.922064</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.077936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  accuracy   roc_auc    pr_auc\n",
       "1       Logistic Regression  1.000000  1.000000  1.000000\n",
       "3                   XGBoost  1.000000  1.000000  1.000000\n",
       "2             Random Forest  0.998596  1.000000  1.000000\n",
       "4                  AdaBoost  0.993154  0.999623  0.995516\n",
       "0  Baseline (Most Frequent)  0.922064  0.500000  0.077936"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# =========================================\n",
    "# 4) Train / Test Split (90/10)\n",
    "# =========================================\n",
    "\n",
    "# 4.1 Feature selection\n",
    "\n",
    "numeric_features = [\n",
    "    \"daily_budget\",\n",
    "    \"hist_spend\", \"hist_clicks\", \"hist_impr\",\n",
    "    \"hist_ctr\", \"hist_cpc\", \"hist_cpm\",\n",
    "    \"day_of_week\", \"month\",\n",
    "    \"name_length\", \"email_length\", \"phone_length\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"project_name\",\n",
    "    \"lead_status\",\n",
    "    \"user_id_str\"\n",
    "]\n",
    "\n",
    "feature_cols = numeric_features + categorical_features\n",
    "\n",
    "X = leads_model[feature_cols].copy()\n",
    "y = leads_model[\"is_converted\"].astype(int)\n",
    "\n",
    "# 4.2 Train/test split – 90% train, 10% test with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape :\", X_test.shape)\n",
    "\n",
    "# =========================================\n",
    "# 5) Preprocessing Pipeline\n",
    "# =========================================\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_transformer = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pos_train = y_train.sum()\n",
    "neg_train = len(y_train) - pos_train\n",
    "scale_pos_weight = neg_train / pos_train\n",
    "print(f\"scale_pos_weight for XGBoost (train only) = {scale_pos_weight:.2f}\")\n",
    "\n",
    "models = {\n",
    "    \"Baseline (Most Frequent)\": DummyClassifier(\n",
    "        strategy=\"most_frequent\",\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight=\"balanced\"   #\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"  \n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=scale_pos_weight  \n",
    "    ),\n",
    "    \"AdaBoost\": AdaBoostClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = []\n",
    "fitted_models = {}\n",
    "probas_test = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n===== Training: {name} =====\")\n",
    "    \n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"model\", clf)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    if hasattr(pipe[\"model\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba = y_pred.astype(float)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_test, y_proba)\n",
    "    except ValueError:\n",
    "        roc = np.nan\n",
    "    try:\n",
    "        pr_auc = average_precision_score(y_test, y_proba)\n",
    "    except ValueError:\n",
    "        pr_auc = np.nan\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"ROC AUC : {roc:.4f}\")\n",
    "    print(f\"PR AUC  : {pr_auc:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": pr_auc\n",
    "    })\n",
    "    \n",
    "    fitted_models[name] = pipe\n",
    "    probas_test[name] = y_proba\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "917c2447-c5a2-41a3-aa1a-917df5582876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (51268, 14)\n",
      "Test shape : (5697, 14)\n",
      "scale_pos_weight for XGBoost (train only) = 11.84\n",
      "\n",
      "===== Training: Baseline (Most Frequent) =====\n",
      "Accuracy: 0.9221\n",
      "ROC AUC : 0.5000\n",
      "PR AUC  : 0.0779\n",
      "\n",
      "===== Training: Logistic Regression =====\n",
      "Accuracy: 0.7416\n",
      "ROC AUC : 0.8657\n",
      "PR AUC  : 0.3741\n",
      "\n",
      "===== Training: Random Forest =====\n",
      "Accuracy: 0.9082\n",
      "ROC AUC : 0.8508\n",
      "PR AUC  : 0.3360\n",
      "\n",
      "===== Training: XGBoost =====\n",
      "Accuracy: 0.8032\n",
      "ROC AUC : 0.8736\n",
      "PR AUC  : 0.3690\n",
      "\n",
      "===== Training: AdaBoost =====\n",
      "Accuracy: 0.9221\n",
      "ROC AUC : 0.7353\n",
      "PR AUC  : 0.2591\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.803230</td>\n",
       "      <td>0.873614</td>\n",
       "      <td>0.368966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.741618</td>\n",
       "      <td>0.865741</td>\n",
       "      <td>0.374136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.908197</td>\n",
       "      <td>0.850844</td>\n",
       "      <td>0.336034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.922064</td>\n",
       "      <td>0.735332</td>\n",
       "      <td>0.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline (Most Frequent)</td>\n",
       "      <td>0.922064</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.077936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  accuracy   roc_auc    pr_auc\n",
       "3                   XGBoost  0.803230  0.873614  0.368966\n",
       "1       Logistic Regression  0.741618  0.865741  0.374136\n",
       "2             Random Forest  0.908197  0.850844  0.336034\n",
       "4                  AdaBoost  0.922064  0.735332  0.259100\n",
       "0  Baseline (Most Frequent)  0.922064  0.500000  0.077936"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# =========================================\n",
    "# 4) Train / Test Split (90/10)\n",
    "# =========================================\n",
    "\n",
    "# 4.1 Feature selection\n",
    "\n",
    "numeric_features = [\n",
    "    \"daily_budget\",\n",
    "    \"hist_spend\", \"hist_clicks\", \"hist_impr\",\n",
    "    \"hist_ctr\", \"hist_cpc\", \"hist_cpm\",\n",
    "    \"day_of_week\", \"month\",\n",
    "    \"name_length\", \"email_length\", \"phone_length\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"project_name\",\n",
    "    \"user_id_str\"\n",
    "]\n",
    "\n",
    "feature_cols = numeric_features + categorical_features\n",
    "\n",
    "X = leads_model[feature_cols].copy()\n",
    "y = leads_model[\"is_converted\"].astype(int)\n",
    "\n",
    "# 4.2 Train/test split – 90% train, 10% test with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape :\", X_test.shape)\n",
    "\n",
    "# =========================================\n",
    "# 5) Preprocessing Pipeline\n",
    "# =========================================\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# لو عندك نسخة قديمة من سكيت-ليرن استخدم sparse بدل sparse_output\n",
    "categorical_transformer = OneHotEncoder(\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pos_train = y_train.sum()\n",
    "neg_train = len(y_train) - pos_train\n",
    "scale_pos_weight = neg_train / pos_train\n",
    "print(f\"scale_pos_weight for XGBoost (train only) = {scale_pos_weight:.2f}\")\n",
    "\n",
    "# =========================================\n",
    "# 6) Models to compare\n",
    "# =========================================\n",
    "\n",
    "models = {\n",
    "    \"Baseline (Most Frequent)\": DummyClassifier(\n",
    "        strategy=\"most_frequent\",\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight=\"balanced\"\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    ),\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    ),\n",
    "    \"AdaBoost\": AdaBoostClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "}\n",
    "\n",
    "results = []\n",
    "fitted_models = {}\n",
    "probas_test = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(f\"\\n===== Training: {name} =====\")\n",
    "    \n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"model\", clf)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    if hasattr(pipe[\"model\"], \"predict_proba\"):\n",
    "        y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba = y_pred.astype(float)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_test, y_proba)\n",
    "    except ValueError:\n",
    "        roc = np.nan\n",
    "    try:\n",
    "        pr_auc = average_precision_score(y_test, y_proba)\n",
    "    except ValueError:\n",
    "        pr_auc = np.nan\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"ROC AUC : {roc:.4f}\")\n",
    "    print(f\"PR AUC  : {pr_auc:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": pr_auc\n",
    "    })\n",
    "    \n",
    "    fitted_models[name] = pipe\n",
    "    probas_test[name] = y_proba\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b3c5f-f057-48ca-a76f-91d3dd627f37",
   "metadata": {},
   "source": [
    "## 7. Model selection and detailed evaluation\n",
    "\n",
    "From the previous comparison, we select **XGBoost** as our primary model:\n",
    "- It achieves the best ROC AUC (~0.90) and competitive PR AUC.\n",
    "- Logistic Regression is kept as a strong, simple baseline.\n",
    "\n",
    "In this section, we will:\n",
    "- Extract predictions and probabilities from XGBoost on the test set.\n",
    "- Compute a classification report (precision, recall, F1).\n",
    "- Inspect the confusion matrix to understand the trade-off between false positives and false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8733ea7f-799b-4307-b94e-cb5a86b4897a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: XGBoost\n",
      "\n",
      "Classification report (threshold = 0.5):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.980     0.803     0.883      5253\n",
      "           1      0.258     0.811     0.391       444\n",
      "\n",
      "    accuracy                          0.803      5697\n",
      "   macro avg      0.619     0.807     0.637      5697\n",
      "weighted avg      0.924     0.803     0.844      5697\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred 0 (Not converted)</th>\n",
       "      <th>Pred 1 (Converted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0 (Not converted)</th>\n",
       "      <td>4216</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1 (Converted)</th>\n",
       "      <td>84</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Pred 0 (Not converted)  Pred 1 (Converted)\n",
       "Actual 0 (Not converted)                    4216                1037\n",
       "Actual 1 (Converted)                          84                 360"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 7.1 Choose best model (XGBoost) and get predictions\n",
    "best_model_name = \"XGBoost\"\n",
    "best_pipe = fitted_models[best_model_name]\n",
    "y_proba_best = probas_test[best_model_name]\n",
    "y_pred_best = (y_proba_best >= 0.5).astype(int)  # default 0.5 threshold\n",
    "\n",
    "print(f\"Using model: {best_model_name}\\n\")\n",
    "\n",
    "# 7.2 Classification report\n",
    "print(\"Classification report (threshold = 0.5):\\n\")\n",
    "print(classification_report(y_test, y_pred_best, digits=3))\n",
    "\n",
    "# 7.3 Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0 (Not converted)\", \"Actual 1 (Converted)\"],\n",
    "    columns=[\"Pred 0 (Not converted)\", \"Pred 1 (Converted)\"]\n",
    ")\n",
    "cm_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec137692-ab04-438a-bffc-7d16b54b2a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576b7c0-d62a-4ef9-930e-a469bc791d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efacbed3-6c4f-4de5-a2a4-ec69743f525c",
   "metadata": {},
   "source": [
    "## 8. Lift and decile analysis\n",
    "\n",
    "To show business value, we want to answer:\n",
    "\n",
    "> “If sales only contact the top 10% of leads ranked by the model, what share of total conversions do they cover?”\n",
    "\n",
    "Steps:\n",
    "- Build a test-set dataframe with the true label and the model score.\n",
    "- Rank leads by score (highest to lowest).\n",
    "- Split them into 10 equal-sized buckets (deciles).\n",
    "- For each decile, compute number of leads, number of conversions, conversion rate, cumulative conversions, and lift vs. the overall average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08951aa9-3013-4c62-9f47-85930bd21df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\AppData\\Local\\Temp\\ipykernel_39476\\4035585299.py:22: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"decile\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leads</th>\n",
       "      <th>conversions</th>\n",
       "      <th>conv_rate</th>\n",
       "      <th>min_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>cum_conversions</th>\n",
       "      <th>cum_perc_conversions</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decile</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570</td>\n",
       "      <td>195</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.774006</td>\n",
       "      <td>0.985849</td>\n",
       "      <td>195</td>\n",
       "      <td>43.918919</td>\n",
       "      <td>4.389580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570</td>\n",
       "      <td>131</td>\n",
       "      <td>0.229825</td>\n",
       "      <td>0.590880</td>\n",
       "      <td>0.771019</td>\n",
       "      <td>326</td>\n",
       "      <td>73.423423</td>\n",
       "      <td>2.948898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569</td>\n",
       "      <td>62</td>\n",
       "      <td>0.108963</td>\n",
       "      <td>0.410388</td>\n",
       "      <td>0.590880</td>\n",
       "      <td>388</td>\n",
       "      <td>87.387387</td>\n",
       "      <td>1.398114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570</td>\n",
       "      <td>24</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.319281</td>\n",
       "      <td>0.410388</td>\n",
       "      <td>412</td>\n",
       "      <td>92.792793</td>\n",
       "      <td>0.540256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570</td>\n",
       "      <td>10</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.257224</td>\n",
       "      <td>0.319222</td>\n",
       "      <td>422</td>\n",
       "      <td>95.045045</td>\n",
       "      <td>0.225107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>569</td>\n",
       "      <td>12</td>\n",
       "      <td>0.021090</td>\n",
       "      <td>0.181518</td>\n",
       "      <td>0.257224</td>\n",
       "      <td>434</td>\n",
       "      <td>97.747748</td>\n",
       "      <td>0.270603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>570</td>\n",
       "      <td>8</td>\n",
       "      <td>0.014035</td>\n",
       "      <td>0.122490</td>\n",
       "      <td>0.181518</td>\n",
       "      <td>442</td>\n",
       "      <td>99.549550</td>\n",
       "      <td>0.180085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>569</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.078623</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>444</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038166</td>\n",
       "      <td>0.078623</td>\n",
       "      <td>444</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.037935</td>\n",
       "      <td>444</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        leads  conversions  conv_rate  min_score  max_score  cum_conversions  \\\n",
       "decile                                                                         \n",
       "1         570          195   0.342105   0.774006   0.985849              195   \n",
       "2         570          131   0.229825   0.590880   0.771019              326   \n",
       "3         569           62   0.108963   0.410388   0.590880              388   \n",
       "4         570           24   0.042105   0.319281   0.410388              412   \n",
       "5         570           10   0.017544   0.257224   0.319222              422   \n",
       "6         569           12   0.021090   0.181518   0.257224              434   \n",
       "7         570            8   0.014035   0.122490   0.181518              442   \n",
       "8         569            2   0.003515   0.078623   0.122378              444   \n",
       "9         570            0   0.000000   0.038166   0.078623              444   \n",
       "10        570            0   0.000000   0.002344   0.037935              444   \n",
       "\n",
       "        cum_perc_conversions      lift  \n",
       "decile                                  \n",
       "1                  43.918919  4.389580  \n",
       "2                  73.423423  2.948898  \n",
       "3                  87.387387  1.398114  \n",
       "4                  92.792793  0.540256  \n",
       "5                  95.045045  0.225107  \n",
       "6                  97.747748  0.270603  \n",
       "7                  99.549550  0.180085  \n",
       "8                 100.000000  0.045100  \n",
       "9                 100.000000  0.000000  \n",
       "10                100.000000  0.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.1 Build evaluation DataFrame on test set\n",
    "eval_df = pd.DataFrame({\n",
    "    \"y_true\": y_test.values,\n",
    "    \"score\": y_proba_best\n",
    "}, index=y_test.index).copy()\n",
    "\n",
    "# Rank by score (highest first) and create deciles 1..10 (1 = highest score)\n",
    "eval_df = eval_df.sort_values(\"score\", ascending=False)\n",
    "eval_df[\"rank\"] = np.arange(1, len(eval_df) + 1)\n",
    "\n",
    "eval_df[\"decile\"] = pd.qcut(\n",
    "    eval_df[\"rank\"],\n",
    "    10,\n",
    "    labels=range(1, 11)  # 1 = top 10%, 10 = bottom 10%\n",
    ")\n",
    "\n",
    "# 8.2 Aggregate by decile\n",
    "overall_conv_rate = eval_df[\"y_true\"].mean()\n",
    "\n",
    "decile_summary = (\n",
    "    eval_df\n",
    "    .groupby(\"decile\")\n",
    "    .agg(\n",
    "        leads=(\"y_true\", \"size\"),\n",
    "        conversions=(\"y_true\", \"sum\"),\n",
    "        conv_rate=(\"y_true\", \"mean\"),\n",
    "        min_score=(\"score\", \"min\"),\n",
    "        max_score=(\"score\", \"max\")\n",
    "    )\n",
    "    .sort_index()  # decile 1 -> 10\n",
    ")\n",
    "\n",
    "# cumulative metrics (from best decile downwards)\n",
    "decile_summary[\"cum_conversions\"] = decile_summary[\"conversions\"].cumsum()\n",
    "total_conversions = decile_summary[\"conversions\"].sum()\n",
    "decile_summary[\"cum_perc_conversions\"] = (\n",
    "    decile_summary[\"cum_conversions\"] / total_conversions * 100\n",
    ")\n",
    "\n",
    "# lift vs global conversion rate\n",
    "decile_summary[\"lift\"] = decile_summary[\"conv_rate\"] / overall_conv_rate\n",
    "\n",
    "decile_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc640ead-fa71-40e0-a55e-b29b4420fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10% of leads (decile 1) contain ~43.9% of all conversions in the test set.\n"
     ]
    }
   ],
   "source": [
    "top10_share = decile_summary.loc[1, \"cum_perc_conversions\"]\n",
    "print(f\"Top 10% of leads (decile 1) contain ~{top10_share:.1f}% of all conversions in the test set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b628b-f1d0-4b47-b836-d0ca43a35fe5",
   "metadata": {},
   "source": [
    "## 9. Lead priority buckets and example leads\n",
    "\n",
    "To make this actionable for sales, we convert model scores into simple priority buckets:\n",
    "\n",
    "- **High priority**: deciles 1–2 (top 20% of scores)\n",
    "- **Medium priority**: deciles 3–6\n",
    "- **Low priority**: deciles 7–10\n",
    "\n",
    "In production, the CRM / dashboard would show a simple *priority tag* next to each lead, powered by the model score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e506a2-18e3-42c9-94e2-a3a668aeb01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_id</th>\n",
       "      <th>project_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>added_date</th>\n",
       "      <th>score</th>\n",
       "      <th>decile</th>\n",
       "      <th>priority</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14088</th>\n",
       "      <td>99975</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-30 14:07:07</td>\n",
       "      <td>0.985849</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14085</th>\n",
       "      <td>99972</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-30 13:58:12</td>\n",
       "      <td>0.985599</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14962</th>\n",
       "      <td>101265</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-08-03 18:53:53</td>\n",
       "      <td>0.985330</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14860</th>\n",
       "      <td>101053</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-08-03 01:14:23</td>\n",
       "      <td>0.985330</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14631</th>\n",
       "      <td>100676</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-08-01 22:28:18</td>\n",
       "      <td>0.985330</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15598</th>\n",
       "      <td>103008</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-08-10 18:34:40</td>\n",
       "      <td>0.985098</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15398</th>\n",
       "      <td>102291</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-08-07 17:16:36</td>\n",
       "      <td>0.985071</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13465</th>\n",
       "      <td>99113</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-27 21:06:29</td>\n",
       "      <td>0.984975</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14833</th>\n",
       "      <td>101004</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-08-02 22:38:52</td>\n",
       "      <td>0.984904</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13258</th>\n",
       "      <td>98746</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-27 03:32:22</td>\n",
       "      <td>0.984807</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13244</th>\n",
       "      <td>98724</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-27 02:29:59</td>\n",
       "      <td>0.984807</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14238</th>\n",
       "      <td>100171</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-31 09:59:19</td>\n",
       "      <td>0.984780</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12441</th>\n",
       "      <td>97414</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-24 17:39:06</td>\n",
       "      <td>0.984780</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12320</th>\n",
       "      <td>97270</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-24 10:00:39</td>\n",
       "      <td>0.984780</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12674</th>\n",
       "      <td>97726</td>\n",
       "      <td>Veranda  Sahl Hasheesh</td>\n",
       "      <td>993</td>\n",
       "      <td>2024-07-25 09:22:18</td>\n",
       "      <td>0.984610</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lead_id            project_name  user_id          added_date     score  \\\n",
       "14088    99975  Veranda  Sahl Hasheesh      993 2024-07-30 14:07:07  0.985849   \n",
       "14085    99972  Veranda  Sahl Hasheesh      993 2024-07-30 13:58:12  0.985599   \n",
       "14962   101265  Veranda  Sahl Hasheesh      993 2024-08-03 18:53:53  0.985330   \n",
       "14860   101053  Veranda  Sahl Hasheesh      993 2024-08-03 01:14:23  0.985330   \n",
       "14631   100676  Veranda  Sahl Hasheesh      993 2024-08-01 22:28:18  0.985330   \n",
       "15598   103008  Veranda  Sahl Hasheesh      993 2024-08-10 18:34:40  0.985098   \n",
       "15398   102291  Veranda  Sahl Hasheesh      993 2024-08-07 17:16:36  0.985071   \n",
       "13465    99113  Veranda  Sahl Hasheesh      993 2024-07-27 21:06:29  0.984975   \n",
       "14833   101004  Veranda  Sahl Hasheesh      993 2024-08-02 22:38:52  0.984904   \n",
       "13258    98746  Veranda  Sahl Hasheesh      993 2024-07-27 03:32:22  0.984807   \n",
       "13244    98724  Veranda  Sahl Hasheesh      993 2024-07-27 02:29:59  0.984807   \n",
       "14238   100171  Veranda  Sahl Hasheesh      993 2024-07-31 09:59:19  0.984780   \n",
       "12441    97414  Veranda  Sahl Hasheesh      993 2024-07-24 17:39:06  0.984780   \n",
       "12320    97270  Veranda  Sahl Hasheesh      993 2024-07-24 10:00:39  0.984780   \n",
       "12674    97726  Veranda  Sahl Hasheesh      993 2024-07-25 09:22:18  0.984610   \n",
       "\n",
       "      decile priority  y_true  \n",
       "14088      1     High       1  \n",
       "14085      1     High       1  \n",
       "14962      1     High       1  \n",
       "14860      1     High       1  \n",
       "14631      1     High       1  \n",
       "15598      1     High       1  \n",
       "15398      1     High       1  \n",
       "13465      1     High       1  \n",
       "14833      1     High       1  \n",
       "13258      1     High       1  \n",
       "13244      1     High       1  \n",
       "14238      1     High       1  \n",
       "12441      1     High       1  \n",
       "12320      1     High       1  \n",
       "12674      1     High       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.1 Map deciles to priority buckets\n",
    "def map_priority(decile):\n",
    "    d = int(decile)\n",
    "    if d <= 2:\n",
    "        return \"High\"\n",
    "    elif d <= 6:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "eval_df[\"priority\"] = eval_df[\"decile\"].apply(map_priority)\n",
    "\n",
    "# 9.2 Join back some lead metadata from leads_model (using the test indices)\n",
    "test_meta = leads_model.loc[eval_df.index, [\n",
    "    \"lead_id\", \"project_name\", \"user_id\", \"added_date\"\n",
    "]].copy()\n",
    "\n",
    "priority_view = pd.concat([test_meta, eval_df[[\"score\", \"decile\", \"priority\", \"y_true\"]]], axis=1)\n",
    "priority_view = priority_view.sort_values(\"score\", ascending=False)\n",
    "\n",
    "# show a few examples\n",
    "priority_view.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1548e134-5e27-4dc2-b3ae-0f0a9e93035c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leads</th>\n",
       "      <th>conversions</th>\n",
       "      <th>conv_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>priority</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>1140</td>\n",
       "      <td>326</td>\n",
       "      <td>28.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>2279</td>\n",
       "      <td>10</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>2278</td>\n",
       "      <td>108</td>\n",
       "      <td>4.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          leads  conversions  conv_rate\n",
       "priority                               \n",
       "High       1140          326      28.60\n",
       "Low        2279           10       0.44\n",
       "Medium     2278          108       4.74"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priority_summary = (\n",
    "    priority_view\n",
    "    .groupby(\"priority\")\n",
    "    .agg(\n",
    "        leads=(\"y_true\", \"size\"),\n",
    "        conversions=(\"y_true\", \"sum\"),\n",
    "        conv_rate=(\"y_true\", \"mean\")\n",
    "    )\n",
    ")\n",
    "priority_summary[\"conv_rate\"] = (priority_summary[\"conv_rate\"] * 100).round(2)\n",
    "priority_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c973e-6ffe-46b8-9bbe-f58b16c05cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf098a6-35e3-47d4-bf98-2b1d4eb14b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c58a33a2-5918-4c78-9d8e-84886e76685d",
   "metadata": {},
   "source": [
    "## 10. Assumptions and clarifying questions\n",
    "\n",
    "### Conversion definition\n",
    "\n",
    "- A lead is considered **converted** if its final recorded status is in:\n",
    "  `[\"DONE_DEAL\", \"ALREADY_BOUGHT\", \"RESALE_REQUEST\", \"MEETING_DONE\", \"HIGH_INTEREST\", \"QUALIFIED\"]`.\n",
    "- We assume these statuses are only assigned **after** a meaningful sales outcome (deal closed, strong intent, or qualified lead).\n",
    "- We treat the latest status per lead as the ground truth and ignore intermediate status changes.\n",
    "\n",
    "### Data and modeling assumptions\n",
    "\n",
    "- The training data is representative of future behaviour (no major changes in marketing strategy or sales process).\n",
    "- Features like `historical spend/clicks/impressions` are already aggregated at campaign/project level and are available at lead creation time.\n",
    "- Date-based features (month, day of week) capture seasonality effects but we did not model long-term trends explicitly.\n",
    "- We removed `lead_status` from the feature set to avoid **target leakage** (the label is derived from this field).\n",
    "\n",
    "### Business questions to clarify in a real project\n",
    "\n",
    "1. **Exact conversion definition**  \n",
    "   - Should we only treat `DONE_DEAL` as a win, or also include `HIGH_INTEREST` / `QUALIFIED`?\n",
    "   - Are there any “fake” deals (test transactions, internal leads) that should be excluded?\n",
    "\n",
    "2. **Time window for conversion**  \n",
    "   - After how many days do we consider a lead “dead”?  \n",
    "   - Do we need a time-to-conversion forecast (e.g. convert in the next 14 days) instead of “ever converted”?\n",
    "\n",
    "3. **Sales capacity and SLA**  \n",
    "   - How many calls/emails can the sales team realistically handle per day?  \n",
    "   - Should we optimise for **precision in the top bucket** (High priority) or for recall overall?\n",
    "\n",
    "4. **Integration into workflow**  \n",
    "   - Where will the score / priority show up? CRM? WhatsApp integration?  \n",
    "   - Do we need an API or a batch scoring job (e.g. daily scoring of new leads)?\n",
    "\n",
    "5. **Monitoring and retraining**  \n",
    "   - How often should we retrain the model (monthly / quarterly)?  \n",
    "   - What KPIs will we track to detect model drift (overall conversion rate, lift in top deciles, etc.)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef0878-f9b0-4435-a4a3-d4c456e2422c",
   "metadata": {},
   "source": [
    " ## 11. How the model would work in production (business view)\n",
    "\n",
    "**Inputs per lead (available at or near creation time):**\n",
    "\n",
    "- Lead-level features:\n",
    "  - Name / email / phone length (proxy for data completeness / seriousness)\n",
    "  - Date features: month, day of week the lead was created\n",
    "- Campaign / project context:\n",
    "  - `project_name`\n",
    "  - `user_id` (customer account / advertiser)\n",
    "- Historical performance signals for that campaign or project:\n",
    "  - `daily_budget`\n",
    "  - Historical spend, clicks, impressions\n",
    "  - Derived ratios: CTR, CPC, CPM\n",
    "\n",
    "**Model output:**\n",
    "\n",
    "- A **conversion probability** between 0 and 1 for each new lead.\n",
    "- A simple **priority tag** based on that score:\n",
    "  - **High priority** → top 20% of scores\n",
    "  - **Medium priority** → next 40%\n",
    "  - **Low priority** → bottom 40%\n",
    "\n",
    "**How sales would use it:**\n",
    "\n",
    "- Every morning, new leads are scored.\n",
    "- Sales reps start with **High priority** leads first (best chance to convert).\n",
    "- Managers can monitor:\n",
    "  - Conversion rate per priority bucket\n",
    "  - Volume and quality of leads per project / campaign\n",
    "- Over time, we can tune thresholds (e.g., only top 10% = High) based on sales capacity and business preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85daa558-ccb0-4aff-a614-66f11f785feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03d83f88-625a-4836-884d-07a401c37788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_date: 2024-06-01\n",
      "max_date: 2025-10-09\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "if \"added_date\" in leads_model.columns:\n",
    "    min_date = leads_model[\"added_date\"].min().date()\n",
    "    max_date = leads_model[\"added_date\"].max().date()\n",
    "elif \"date\" in leads_model.columns:\n",
    "    min_date = pd.to_datetime(leads_model[\"date\"]).min().date()\n",
    "    max_date = pd.to_datetime(leads_model[\"date\"]).max().date()\n",
    "else:\n",
    "    min_date = date(2024, 1, 1)\n",
    "    max_date = date(2024, 12, 31)\n",
    "\n",
    "print(\"min_date:\", min_date)\n",
    "print(\"max_date:\", max_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fb3bf-b347-4cb2-8ad4-ac1d73d5b5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4172a8e-f323-4201-a829-5dc34eab4397",
   "metadata": {},
   "source": [
    "# Joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "052131e5-0004-4754-9314-edabacbc3a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_date: 2024-06-01\n",
      "max_date: 2025-10-09\n",
      "Champion model (forced): XGBoost\n",
      "accuracy     0.80323\n",
      "roc_auc     0.873614\n",
      "pr_auc      0.368966\n",
      "Name: 3, dtype: object\n",
      "Saved artifact to leadsmart_champion.joblib ✅\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "from datetime import date\n",
    "\n",
    "# =========================================\n",
    "# 1) Compute min_date / max_date from leads_model\n",
    "# =========================================\n",
    "if \"added_date\" in leads_model.columns:\n",
    "    min_date = leads_model[\"added_date\"].min().date()\n",
    "    max_date = leads_model[\"added_date\"].max().date()\n",
    "elif \"date\" in leads_model.columns:\n",
    "    min_date = pd.to_datetime(leads_model[\"date\"]).min().date()\n",
    "    max_date = pd.to_datetime(leads_model[\"date\"]).max().date()\n",
    "else:\n",
    "    # Fallback \n",
    "    min_date = date(2024, 1, 1)\n",
    "    max_date = date(2024, 12, 31)\n",
    "\n",
    "print(\"min_date:\", min_date)\n",
    "print(\"max_date:\", max_date)\n",
    "\n",
    "# =========================================\n",
    "# 2) Force champion model = XGBoost\n",
    "# =========================================\n",
    "# \n",
    "if \"XGBoost\" not in fitted_models:\n",
    "    raise ValueError(\"XGBoost model is not found in fitted_models dict.\")\n",
    "\n",
    "best_model_name = \"XGBoost\"\n",
    "champion = fitted_models[best_model_name]\n",
    "\n",
    "# \n",
    "xgb_row = results_df[results_df[\"model\"] == best_model_name].iloc[0]\n",
    "\n",
    "print(\"Champion model (forced):\", best_model_name)\n",
    "print(xgb_row[[\"accuracy\", \"roc_auc\", \"pr_auc\"]])\n",
    "\n",
    "# =========================================\n",
    "# 3) Info for Streamlit artifact\n",
    "# =========================================\n",
    "neg_count = (leads_model[\"is_converted\"] == 0).sum()\n",
    "pos_count = (leads_model[\"is_converted\"] == 1).sum()\n",
    "\n",
    "artifact = {\n",
    "    # pipeline (preprocessor + XGBoost)\n",
    "    \"model\": champion,\n",
    "    \n",
    "   \n",
    "    \"best_model_name\": best_model_name,\n",
    "    \n",
    "    \"numeric_features\": numeric_features,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \n",
    "    \"conversion_statuses\": conversion_statuses,\n",
    "    \n",
    "    \"min_date\": min_date,\n",
    "    \"max_date\": max_date,\n",
    "    \n",
    "    # class balance\n",
    "    \"class_balance\": {\n",
    "        \"neg\": int(neg_count),\n",
    "        \"pos\": int(pos_count),\n",
    "    },\n",
    "    \n",
    "    \"metrics\": results_df,\n",
    "    \n",
    "    \"project_options\": sorted(\n",
    "        leads_model[\"project_name\"].dropna().unique().tolist()\n",
    "    ),\n",
    "    \"user_options\": sorted(\n",
    "        leads_model[\"user_id_str\"].dropna().unique().tolist()\n",
    "    ),\n",
    "    \n",
    "    \"median_numeric\": leads_model[numeric_features].median().to_dict(),\n",
    "}\n",
    "\n",
    "# =========================================\n",
    "# 4) Save artifact\n",
    "# =========================================\n",
    "dump(artifact, \"leadsmart_champion.joblib\")\n",
    "\n",
    "print(\"Saved artifact to leadsmart_champion.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0e11c39-4904-444d-b081-5d673883b83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\Downloads\\task hamada\\test_data\n",
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 444E-EEC7\n",
      "\n",
      " Directory of C:\\Users\\LOQ\\Downloads\\task hamada\\test_data\n",
      "\n",
      "12/08/2025  06:22 AM           423,525 leadsmart_champion.joblib\n",
      "               1 File(s)        423,525 bytes\n",
      "               0 Dir(s)  41,760,702,464 bytes free\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "!dir *.joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917208e-6698-4ad2-a347-991260c223a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
